{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxTh9+9z+sGIfUAzgVfhFM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<font size=\"+3\"><strong>Machine Learning: Core Concepts</strong></font>"],"metadata":{"id":"S5yebtr-imIY"}},{"cell_type":"markdown","source":["# Statistical Concepts"],"metadata":{"id":"Iy48gb-sio1X"}},{"cell_type":"markdown","source":["## ***Cost Functions***\n","\n","When we train a model, we're solving an optimization problem. We provide training data to an algorithm and tell it to find the model or model parameters that best fit the data. But how can the algorithm judge what the \"best\" fit is? What criteria should it use?\n","\n","A **cost function** (sometimes also called a loss or error function) is a mathematical formula that provides the score by which the algorithm will determine the best fit. Generally, the goal is to minimize the cost function and get the lowest score. For linear models, these functions measure distance, and the model tries to to get the closest fit to the data. For tree-based models, they measure impurity, and the model tries to get the most terminal nodes.\n","\n","#### Key Points about Cost Functions:\n","\n","1. **Quantifying Error**: The cost function assigns a numerical value to the error between predicted values and actual values. It depends on the problem type.\n","\n","2. **Model Complexity**: The choice of cost function can also influence the complexity of the resulting model. Some cost functions encourage simpler models that generalize better, while others might lead to more complex models that overfit the training data.\n","\n","3. **Trade-offs**:  The choice of a specific cost function can involve trade-offs. For example, a complex cost function might better capture the intricacies of the problem, but it could also make optimization more difficult. A simpler cost function might lead to faster training, but it might not capture all the nuances of the data.\n","\n","4. **Common Cost Functions**:\n","   - **Mean Squared Error (MSE)**: For regression, it measures squared differences.\n","\n","   $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n","\n","   Where:\n","    - $\\(n\\)$ is the number of data points\n","    - $\\(y_i\\)$ is the true target value for the $\\(i\\)$th data point\n","    - $\\(\\hat{y}_i\\)$ is the predicted value for the $\\(i\\)$th data point\n","\n","\n","   - **Cross-Entropy**: Often used for classification problems. It quantifies the difference between predicted class probabilities and true class labels.\n","\n","   - **Hinge Loss**: Used for support vector machines (SVMs) and other classifiers. It aims to maximize the margin between classes.\n","\n","\n","\n","   - **Log-Likelihood**: Used in probabilistic models. It measures the likelihood of the observed data under the model's distribution.\n","\n","************************************"],"metadata":{"id":"-kqySBBRjCDu"}},{"cell_type":"code","source":[],"metadata":{"id":"poLGQsgYq4yY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Residuals in Regression Analysis***\n","\n","When we perform regression analysis to model relationships between variables, we often use a line (or curve) of best fit to approximate the underlying trend in the data. However, real-world data is rarely perfectly clean and follows the theoretical model exactly. Residuals come into play when we consider the differences between the observed data points and the predictions made by the regression model.\n","\n","### What are Residuals?\n","\n","A **residual** is the vertical distance between an individual data point and the regression line. In other words, it's the difference between the actual observed value and the value predicted by the regression model. Each data point has its own residual, and it tells us how much that particular data point deviates from the regression line.\n","\n","### Interpreting Residuals\n","\n","Residuals provide valuable insights into the quality and appropriateness of our regression model. Here's how to interpret them:\n","\n","- **Positive Residual**: If a residual is positive, it means that the actual observed value is higher than the value predicted by the regression line at that particular point. This suggests that the model is underestimating the value for that data point.\n","\n","- **Negative Residual**: Conversely, a negative residual indicates that the actual observed value is lower than the predicted value. In this case, the model is overestimating the value.\n","\n","- **Zero Residual**: If a data point lies exactly on the regression line, its residual is zero. This means that the model's prediction matches the observed value perfectly for that point.\n","\n","### Importance of Residuals\n","\n","Residual analysis is an essential part of regression modeling for several reasons:\n","\n","1. **Model Validation**: By examining the distribution of residuals, we can assess how well the regression model fits the data. If the residuals are randomly scattered around zero and have constant variance, it's an indication that the model is appropriate.\n","\n","2. **Detection of Patterns**: Patterns in the residual plot, such as a curved shape or a fan-like pattern, might indicate that the model is not capturing some underlying relationships between variables.\n","\n","3. **Outlier Detection**: Residuals that are significantly larger or smaller than others could represent outliers or anomalies in the data. Outliers can have a substantial impact on the regression model's performance.\n","************************************\n"],"metadata":{"id":"UzvV1h29nLYP"}},{"cell_type":"code","source":[],"metadata":{"id":"jL4Wiiruq6EX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Performance Metrics***\n","\n","In statistics, an *error* is the difference between a measurement and reality. There may not be any difference at all, but there's usually *something* not quite right, and we need to account for that in our model. To do that, we need to figure out the **mean absolute error (MAE)**. Absolute error is the error in a single measurement, and mean absolute error is the average error over the course of several measurements.\n","\n","## More Performance Metrics\n","\n","### Mean Squared Error (MSE)\n","\n","The **mean squared error (MSE)** is another widely used performance metric, especially in regression tasks. It measures the average squared difference between the predicted values and the true values. MSE gives more weight to larger errors, making it sensitive to outliers.\n","\n","$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n","\n","### Root Mean Squared Error (RMSE)\n","\n","The **root mean squared error (RMSE)** is a modification of the MSE that takes the square root of the average squared differences. RMSE provides an estimate of the standard deviation of the prediction errors.\n","\n","$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n","\n","\n","### R-squared (Coefficient of Determination)\n","\n","The **R-squared** value, also known as the coefficient of determination, quantifies the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. It ranges between 0 and 1, where 1 indicates a perfect fit.\n","$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}i)^2}{\\sum{i=1}^{n} (y_i - \\bar{y})^2}$$\n","\n","These metrics are commonly used for classification tasks:\n","\n","- **Accuracy**: Measures the proportion of correctly classified instances out of the total instances.\n","- **Precision**: Measures the proportion of true positive predictions out of all positive predictions.\n","- **Recall**: Measures the proportion of true positive predictions out of all actual positive instances.\n","- **F1-Score**: Combines precision and recall to provide a balanced measure of a model's performance.\n","************************************"],"metadata":{"id":"OZdK13CXouYX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SU6100Uqhy_k"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Data Concepts"],"metadata":{"id":"QPrBc2sWq9qv"}},{"cell_type":"markdown","source":["## ***Leakage***\n","\n","**Leakage** is the use of data in training your model that would not be typically be available when making predictions. For example, suppose we want to predict property prices in USD but include property prices in Mexican Pesos in our model. If we assume a fixed exchange rate or a nearly constant exchange rate, then our model will have a low error on the training data, but this will not be reflective of its performance on real world data.\n","\n","**Leakage** refers to the inadvertent introduction of information into your training data that would not typically be available when making predictions on real-world data. It can significantly distort the performance of your model during training and evaluation, leading to overly optimistic results that do not generalize well to unseen data.\n","\n","### Types of Leakage\n","\n","There are two main types of leakage to be aware of:\n","\n","1. **Target Leakage**: This occurs when information from the target variable is available to the model during training, but it would not be available at prediction time. Including such information can make your model appear more accurate during training, but it will perform poorly in real-world scenarios.\n","\n","   Example: Imagine you're predicting whether a credit card transaction is fraudulent or not. If your model has access to information about whether a transaction was later marked as fraudulent during training, it would lead to target leakage. The model could pick up on patterns that are not present in real-world situations.\n","\n","2. **Data Leakage**: This happens when your model has access to features that it should not have during training. These features could be directly or indirectly related to the target variable.\n","\n","   Example: Suppose you're predicting the stock market, and you include future stock prices as features in your training data. This would lead to data leakage because, in a real-world scenario, you wouldn't have access to future stock prices at the time of prediction.\n","\n","### Consequences of Leakage\n","\n","Leakage can have several negative consequences:\n","\n","- **Overfitting**: Your model could learn to exploit the leaked information to fit the training data extremely well, resulting in poor generalization to new data.\n","  \n","- **Unrealistic Performance**: Leakage can make your model appear highly accurate during training and validation, but its performance will degrade significantly when faced with real-world scenarios.\n","\n","### Preventing Leakage\n","\n","To prevent leakage, follow these practices:\n","\n","1. **Data Splitting**: Always split your data into training, validation, and testing sets before preprocessing or feature engineering. Leakage can occur if these steps are performed on the entire dataset before splitting.\n","\n","2. **Feature Engineering**: Avoid including any information that wouldn't be available during prediction. For instance, remove features derived from the target variable or future data.\n","\n","3. **Time-Based Splits**: If dealing with time-series data, ensure that your validation and test sets come after your training data. This prevents future information from leaking into the training process.\n","\n"],"metadata":{"id":"CqdFyanLrAoH"}},{"cell_type":"code","source":[],"metadata":{"id":"8DwlP4C0zSWi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Imputation***\n","\n","Datasets are often incomplete, containing missing values in one or more rows or columns. When dealing with these missing entries, it's important to address them appropriately to ensure accurate and reliable analysis. **Imputation** is the process of estimating and filling in these missing values with educated guesses.\n","\n","### Importance of Imputation\n","\n","Imputation is crucial for several reasons:\n","\n","1. **Preserving Data Integrity**: Removing rows or columns with missing values can lead to loss of valuable information. Imputation allows you to retain as much data as possible.\n","\n","2. **Maintaining Statistical Power**: Imputing missing values helps ensure that your analysis has sufficient statistical power, preventing underestimation of variance or biases in results.\n","\n","3. **Algorithm Compatibility**: Many machine learning algorithms require complete datasets. Imputation allows you to use these algorithms effectively.\n","\n","### Imputation Techniques\n","\n","There are various techniques for imputing missing values:\n","\n","1. **Mean/Median Imputation**: Replacing missing values with the mean or median of the non-missing values in the column. This is a simple method but assumes the data is missing at random.\n","\n","2. **Mode Imputation**: Replacing missing categorical values with the mode (most frequent value) of the non-missing values in the column.\n","\n","3. **Regression Imputation**: Predicting missing values using regression models based on other variables.\n","\n","4. **K-Nearest Neighbors (KNN) Imputation**: Filling in missing values by considering the values of the k-nearest neighbors.\n","\n","5. **Extrapolation**: For time-series data, using previous or subsequent values to estimate missing values based on patterns.\n","\n","### Caveats of Imputation\n","\n","While imputation is valuable, it's important to be cautious:\n","\n","- **Bias Introductions**: Imputation can introduce bias if not done carefully. The imputed values might not accurately represent the true values.\n","- **Underestimation of Uncertainty**: Imputed values often underestimate the uncertainty associated with missing data.\n","- **Distorted Relationships**: Imputed data might distort the relationships between variables, impacting downstream analyses.\n","\n"],"metadata":{"id":"cLLBTdCwzSoS"}},{"cell_type":"markdown","source":["[Sklearn-Impuation](https://scikit-learn.org/stable/modules/impute.html#impute)"],"metadata":{"id":"rFNuF6I_z7vK"}},{"cell_type":"code","source":[],"metadata":{"id":"As-4hn5u0L7b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Generalization***\n","\n","Notice that we tested the model with a dataset that's *different* from the one we used to train the model. Machine learning models are useful if they allow you to make predictions about data other than what you used to train your model. We call this concept **generalization**. By testing your model with different data than you used to train it, you're checking to see if your model can generalize. Most machine learning models do not generalize to all possible types of input data, so they should be used with care. On the other hand, machine learning models that don't generalize to make predictions for at least a restricted set of data aren't very useful.\n","\n","### The Goal of Generalization\n","\n","The ultimate goal of machine learning is to create models that can generalize well to unseen data. A model that generalizes well can accurately predict outcomes for new and unseen examples, even if those examples come from a different distribution than the training data. Generalization ensures that your model is not merely memorizing the training data but is learning meaningful patterns that apply more broadly.\n","\n","### Overfitting and Underfitting\n","\n","Two common challenges related to generalization are **overfitting** and **underfitting**:\n","\n","- **Overfitting**: Occurs when a model learns the noise and random fluctuations in the training data rather than the underlying patterns. As a result, the model performs well on the training data but poorly on new, unseen data.\n","\n","- **Underfitting**: Happens when a model is too simple to capture the underlying patterns in the data. It performs poorly on both the training data and new data.\n","\n","### Techniques for Generalization\n","\n","To achieve better generalization, consider the following techniques:\n","\n","1. **Cross-Validation**: Use cross-validation to assess your model's performance on multiple subsets of the data. This helps evaluate how well your model generalizes.\n","\n","2. **Regularization**: Apply techniques like L1 or L2 regularization to prevent overfitting by adding penalty terms to the model's loss function.\n","\n","3. **Feature Selection**: Choose relevant features and remove irrelevant ones to prevent the model from fitting noise.\n","\n","4. **Ensemble Methods**: Combine multiple models to improve overall performance and reduce overfitting.\n","\n","### Balancing Generalization and Complexity\n","\n","Striking the right balance between a model's complexity and its ability to generalize is key. A model that's too complex might overfit, while one that's too simple might underfit. Regularization techniques and model evaluation help find this balance.\n"],"metadata":{"id":"8Br_HqnI0Mai"}},{"cell_type":"markdown","source":["# Model Concepts"],"metadata":{"id":"plhKoKkO1dYK"}},{"cell_type":"markdown","source":["## Hyperparameters\n","\n","When we instantiate an estimator, we can pass keyword arguments that will dictate its structure. These arguments are called **hyperparameters**. For example, when we defined our decision tree estimator, we chose how many layers the tree would have using the `max_depth` keyword. This is in contrast to **parameters**, which are the numbers that our model uses to make predictions based on features. Parameters are optimized during the training process based on data and input features. They keep changing during training to fit the data, and only the best-performing ones are selected.\n","\n","### The Distinction between Parameters and Hyperparameters\n","\n","**Parameters** are learned by the model during training. They are the coefficients or weights that are adjusted to minimize the difference between the model's predictions and the actual target values. For example, in linear regression, the coefficients of the features are parameters.\n","\n","**Hyperparameters**, on the other hand, are set before the training process begins. They determine the high-level characteristics of the model, such as its complexity, capacity, or behavior during training. Adjusting hyperparameters impacts how the model learns and generalizes.\n","\n","### Role of Hyperparameters\n","\n","Hyperparameters play a crucial role in machine learning:\n","\n","1. **Model Behavior**: Hyperparameters influence how a model learns and generalizes. They determine the complexity of the model, which affects its ability to fit the training data and generalize to new data.\n","\n","2. **Avoiding Overfitting**: By adjusting hyperparameters, you can control overfitting, underfitting, and the balance between the two. Techniques like regularization involve tuning hyperparameters.\n","\n","3. **Computational Efficiency**: Some hyperparameters affect how the model is trained and how computations are distributed across resources.\n","\n","### Common Hyperparameters\n","\n","Here are some common examples of hyperparameters and their impact on models:\n","\n","- **Learning Rate**: Affects the step size taken during gradient descent in optimization algorithms.\n","\n","- **Number of Neighbors (K) in K-Nearest Neighbors**: Influences the level of local vs. global pattern capturing.\n","\n","- **Number of Trees in Random Forest**: Determines the trade-off between model complexity and ensemble strength.\n","\n","- **Regularization Strength**: Affects the penalty applied to large coefficients in linear models.\n","\n","### Hyperparameter Tuning\n","\n","Finding the right set of hyperparameters is crucial for model performance. This process is called **hyperparameter tuning**. It involves selecting the best hyperparameters based on validation data or cross-validation. Grid search, random search, and Bayesian optimization are common methods used for tuning.\n"],"metadata":{"id":"hD70gmg91l75"}}]}