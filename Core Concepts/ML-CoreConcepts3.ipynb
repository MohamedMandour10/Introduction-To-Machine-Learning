{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9pBgqCk0lEtMxzIVfo+Ss"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PolynomialFeatures\n","\n","The `PolynomialFeatures` class in scikit-learn is a preprocessing technique used to generate polynomial features from the original features of a dataset. It transforms an input feature matrix into a new feature matrix, wherein the new features are polynomial combinations of the original features up to a specified degree.\n","\n","#### Usage:\n","\n","```python\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# Set the degree of the polynomial\n","DEGREE = 2\n","\n","# Initialize PolynomialFeatures with the specified degree\n","poly = PolynomialFeatures(degree=DEGREE)\n","\n","# Transform the original feature matrix to polynomial features\n","X_poly = poly.fit_transform(X)\n","```\n","\n","#### Parameters:\n","- `degree`: An integer indicating the degree of the polynomial. It determines the maximum degree of the polynomial features to be generated. For example, if `degree=2`, it will generate polynomial features up to the second degree (e.g., x^2, x1*x2).\n","\n","#### Attributes:\n","- `powers_`: An array representing the powers of the features in each polynomial term. It provides information about which original features are combined to create each polynomial feature.\n","\n","#### Example:\n","Suppose you have a dataset with a single feature `X`:\n","```\n","X = [[a],\n","     [b],\n","     [c]]\n","```\n","Applying `PolynomialFeatures` with `degree=2` would transform the feature matrix into:\n","```\n","[[1, a, a^2],\n"," [1, b, b^2],\n"," [1, c, c^2]]\n","```\n","\n","#### Use Cases:\n","- Polynomial regression: By generating polynomial features, you can fit a polynomial curve to the data using linear regression.\n","- Nonlinear relationships: Polynomial features can capture nonlinear relationships between features and the target variable.\n","\n","#### Considerations:\n","- Increasing the degree of the polynomial can lead to a higher number of features, potentially causing overfitting, especially with a small dataset.\n","- It's essential to balance the complexity of the model with its generalization performance when selecting the degree of the polynomial.\n","\n"],"metadata":{"id":"GMk1tIHH82LR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBJzS9Tx7-GN"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Cross-Validation\n","\n","<img src=\"https://www.ejable.com/wp-content/uploads/2022/04/steps-for-K-fold-Cross-Validation.webp\" alt=\"Cross-Validation\" width=\"600\" height=\"500\">\n","\n","Cross-validation is a resampling technique used to assess the performance of a machine learning model and to mitigate issues such as overfitting. It involves partitioning the dataset into subsets, performing training and evaluation multiple times, and averaging the results to obtain a more robust estimate of the model's performance.\n","\n","#### Usage:\n","\n","```python\n","from sklearn.model_selection import cross_val_score, KFold\n","\n","# Initialize the cross-validation method (e.g., KFold)\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Perform cross-validation with a specified model and dataset\n","scores = cross_val_score(model, X, y, cv=kf)\n","```\n","\n","#### Parameters:\n","- `n_splits`: An integer indicating the number of folds (or subsets) into which the dataset is divided for cross-validation.\n","- `shuffle`: A boolean indicating whether to shuffle the data before splitting into folds. Shuffling helps in randomizing the data, especially useful when the dataset has inherent order or grouping.\n","- `random_state`: An integer or `RandomState` instance, used for reproducibility. It controls the randomization applied to the data before splitting into folds.\n","\n","#### Methods:\n","- `cross_val_score`: Computes the scores of a specified model on different cross-validation folds and returns an array of scores.\n","- Other cross-validation methods like `KFold`, `StratifiedKFold`, `LeaveOneOut`, etc., can be used to customize the cross-validation strategy based on specific requirements.\n","\n","#### Use Cases:\n","- Model evaluation: Cross-validation provides a more reliable estimate of a model's performance compared to a single train-test split.\n","- Hyperparameter tuning: It helps in selecting the best hyperparameters for the model by evaluating its performance across different parameter combinations.\n","\n","#### Considerations:\n","- Cross-validation can be computationally expensive, especially for large datasets or complex models, as it involves training the model multiple times.\n","- It's important to use an appropriate number of folds and ensure randomness in data shuffling to obtain unbiased estimates of the model's performance.\n","\n"],"metadata":{"id":"OBYtPCiy917h"}},{"cell_type":"code","source":[],"metadata":{"id":"ao0BSCWn-xWZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Nested Cross-Validation\n","\n","Nested cross-validation is an extension of traditional cross-validation that is used to evaluate the performance of a machine learning model while also tuning hyperparameters. It involves an outer loop for model evaluation and an inner loop for hyperparameter tuning. Nested cross-validation provides a more reliable estimate of a model's performance, especially when dealing with small datasets or when hyperparameters need to be optimized.\n","\n","#### Usage:\n","\n","```python\n","from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n","\n","# Define the parameter grid for hyperparameter tuning\n","param_grid = {'C': [0.1, 1, 10]}\n","\n","# Initialize the outer cross-validation method\n","outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Initialize the inner cross-validation method\n","inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n","\n","# Initialize the GridSearchCV object\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv)\n","\n","# Perform nested cross-validation\n","nested_scores = cross_val_score(grid_search, X, y, cv=outer_cv)\n","```\n","\n","#### Parameters:\n","- `param_grid`: A dictionary specifying the hyperparameters and their possible values for tuning.\n","- `outer_cv`: The outer cross-validation method used to evaluate the model's performance.\n","- `inner_cv`: The inner cross-validation method used for hyperparameter tuning.\n","- `estimator`: The model or estimator to be evaluated and tuned.\n","- `cv`: The cross-validation strategy used for evaluating the model performance during outer loop iterations.\n","\n","#### Methods:\n","- `GridSearchCV`: Performs an exhaustive search over the hyperparameter grid specified in `param_grid` and selects the best combination of hyperparameters based on the inner cross-validation scores.\n","- `cross_val_score`: Computes the scores of the model using nested cross-validation, providing an array of scores representing the model's performance across different outer folds.\n","\n","#### Use Cases:\n","- Model evaluation with hyperparameter tuning: Nested cross-validation provides an unbiased estimate of a model's performance while also optimizing hyperparameters, thus avoiding overfitting.\n","- Comparative model evaluation: It allows comparing the performance of different models with optimized hyperparameters in a fair and unbiased manner.\n","\n","#### Considerations:\n","- Nested cross-validation can be computationally expensive, especially for large datasets or complex models, as it involves multiple iterations of both inner and outer loops.\n","- It's essential to use an appropriate number of folds and ensure randomness in data shuffling for both inner and outer loops to obtain reliable estimates of model performance.\n","\n"],"metadata":{"id":"nmY-PYgK-gmD"}},{"cell_type":"code","source":[],"metadata":{"id":"b7nre8jSAIzg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Decision Boundary\n","\n","![Decision Boundary](https://i.stack.imgur.com/Dua5N.png)\n","\n","The decision boundary is a crucial concept in classification problems, especially in machine learning. It represents the dividing line that separates different classes or categories in the feature space. The decision boundary is determined by the model's learned parameters and defines the regions where different classes are predicted.\n","\n","#### Usage:\n","\n","```python\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Define a mesh grid to visualize the decision boundary\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    h = 0.01  # step size in the mesh\n","\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","    # Predict the function value for the whole grid\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n","    plt.xlabel('Feature 1')\n","    plt.ylabel('Feature 2')\n","    plt.title('Decision Boundary')\n","    plt.show()\n","\n","# Call the function to plot decision boundary\n","plot_decision_boundary(model, X_train, y_train)\n","```\n","\n","#### Parameters:\n","- `model`: The trained classifier or model used to make predictions.\n","- `X`: The feature matrix representing the input data.\n","- `y`: The target vector containing the class labels.\n","\n","#### Functionality:\n","- The `plot_decision_boundary` function takes a trained model along with the input features (`X`) and corresponding labels (`y`) as input.\n","- It creates a mesh grid covering the entire feature space with a specified step size (`h`).\n","- Using the trained model, it predicts the class labels for each point on the mesh grid.\n","- Finally, it plots the decision boundary along with the training examples to visualize the classification regions.\n","\n","#### Visualization:\n","- The decision boundary is visualized as a contour plot that separates different classes in the feature space.\n","- Training examples are often overlaid on the plot to provide context and demonstrate how the decision boundary separates the classes.\n","\n"],"metadata":{"id":"cGHA31jUAJDY"}},{"cell_type":"code","source":[],"metadata":{"id":"9Xpkq8WnBXBw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Scaling\n","\n","<img src=\"https://www.pickl.ai/blog/wp-content/uploads/2023/08/Feature-Scaling-in-Machine-Learning.jpg\" alt=\"Feature Scaling\" width=\"500\" height=\"400\">\n","\n","Feature scaling is a preprocessing technique used to standardize or normalize the range of independent variables or features in a dataset. It ensures that all features have the same scale, preventing features with larger magnitudes from dominating those with smaller magnitudes during model training. Feature scaling is especially important for algorithms that rely on distance-based calculations or gradient descent optimization.\n","\n","#### Usage:\n","\n","```python\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","\n","# Initialize the scalers\n","standard_scaler = StandardScaler()\n","min_max_scaler = MinMaxScaler()\n","robust_scaler = RobustScaler()\n","\n","# Perform feature scaling\n","X_train_standardized = standard_scaler.fit_transform(X_train)\n","X_train_min_max_scaled = min_max_scaler.fit_transform(X_train)\n","X_train_robust_scaled = robust_scaler.fit_transform(X_train)\n","```\n","\n","#### Techniques:\n","\n","1. **StandardScaler**:\n","   - Standardizes features by removing the mean and scaling to unit variance.\n","   - Suitable for normally distributed data and when the standard deviation is expected to be small.\n","   Formula:\n","   - StandardScaler formula: $$x_{scaled} = \\frac{x - \\mu}{\\sigma}$$\n","\n","2. **MinMaxScaler**:\n","   - Scales features to a specified range (default: [0, 1]).\n","   - Preserves the shape of the original distribution and is less affected by outliers.\n","  - Formula: MinMaxScaler formula: $$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n","\n","3. **RobustScaler**:\n","   - Scales features using statistics that are robust to outliers (e.g., median and interquartile range).\n","   - Ideal for datasets with outliers or non-normal distributions.\n","   - Formula: RobustScaler formula: $$x_{scaled} = \\frac{x - Q_1(x)}{Q_3(x) - Q_1(x)}$$\n","\n","#### Parameters:\n","- `X_train`: The feature matrix representing the input data.\n","\n","#### Functionality:\n","- Each scaler is initialized, and then the `fit_transform` method is applied to the training data to compute the scaling parameters and transform the features simultaneously.\n","\n","#### Considerations:\n","- Feature scaling should be performed separately on training and test datasets to prevent data leakage.\n","- The choice of scaler depends on the characteristics of the data and the requirements of the algorithm being used.\n","\n"],"metadata":{"id":"XKnjE3SOBXP3"}}]}